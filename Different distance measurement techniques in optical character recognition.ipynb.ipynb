{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80feedc6-083a-4837-9ebb-f56e384c62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d0521b-7b07-4fa2-acc1-f9931ed8fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing and preprocessing Amharic character sample dataset\n",
    "def preprocess_images(input_folder, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate through each subfolder (character class)\n",
    "    for char_folder in os.listdir(input_folder):\n",
    "        char_folder_path = os.path.join(input_folder, char_folder)\n",
    "\n",
    "        if os.path.isdir(char_folder_path):\n",
    "            output_char_folder = os.path.join(output_folder, char_folder)\n",
    "\n",
    "            # Create the output subfolder for the character class\n",
    "            if not os.path.exists(output_char_folder):\n",
    "                os.makedirs(output_char_folder)\n",
    "\n",
    "            # Iterate through each image in the character class subfolder\n",
    "            for filename in os.listdir(char_folder_path):\n",
    "                if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    # Read the image using Pillow\n",
    "                    image_path = os.path.join(char_folder_path, filename)\n",
    "\n",
    "                    img = Image.open(image_path).convert('L')  # Open and convert to grayscale\n",
    "                    img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "\n",
    "                    # Save the preprocessed image to the output subfolder using Pillow\n",
    "                    output_path = os.path.join(output_char_folder, filename)\n",
    "                    img.save(output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'input_folder' and 'output_folder' with your actual paths\n",
    "    input_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset\"\n",
    "    output_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Preprocessed\"\n",
    "\n",
    "    preprocess_images(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e5944e6-b643-47e8-a968-f62578a68fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processed images meet the criteria.\n"
     ]
    }
   ],
   "source": [
    "# Ensuring uniformity in size, resolution, and grayscale conversion of the above preprocessed data set\n",
    "def check_uniformity(folder_path):\n",
    "    # Keep track of whether all processed images meet the criteria\n",
    "    all_meet_criteria = True\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            image_path = os.path.join(root, file)\n",
    "\n",
    "            # Open the preprocessed image\n",
    "            img = Image.open(image_path)\n",
    "\n",
    "            # Check size uniformity\n",
    "            width, height = img.size\n",
    "\n",
    "            # Check resolution uniformity\n",
    "            resolution = img.info.get(\"dpi\", (0, 0))\n",
    "\n",
    "            # Check grayscale conversion\n",
    "            if img.mode != \"L\":\n",
    "                all_meet_criteria = False\n",
    "                print(f\"Image: {file} is not grayscale.\")\n",
    "            elif width != 28 or height != 28:  # Adjust these dimensions based on your requirements\n",
    "                all_meet_criteria = False\n",
    "                print(f\"Image: {file} does not have the required size.\")\n",
    "\n",
    "    if all_meet_criteria:\n",
    "        print(\"All processed images meet the criteria.\")\n",
    "\n",
    "# Specify the path to the preprocessed images\n",
    "preprocessed_folder_path = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Preprocessed\"\n",
    "\n",
    "# Perform uniformity checks\n",
    "check_uniformity(preprocessed_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25ecf635-1556-4e97-9b0e-2fc107156033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test set\n",
    "def split_data(input_folder, output_train_folder, output_test_folder, test_size=0.4, random_state=42):\n",
    "    # Create output folders if they don't exist\n",
    "    os.makedirs(output_train_folder, exist_ok=True)\n",
    "    os.makedirs(output_test_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through each subfolder (character class)\n",
    "    for char_folder in os.listdir(input_folder):\n",
    "        char_folder_path = os.path.join(input_folder, char_folder)\n",
    "\n",
    "        if os.path.isdir(char_folder_path):\n",
    "            # Split images into training and testing sets\n",
    "            train_folder = os.path.join(output_train_folder, char_folder)\n",
    "            test_folder = os.path.join(output_test_folder, char_folder)\n",
    "\n",
    "            os.makedirs(train_folder, exist_ok=True)\n",
    "            os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "            images = os.listdir(char_folder_path)\n",
    "            train_images, test_images = train_test_split(images, test_size=test_size, random_state=random_state)\n",
    "\n",
    "            # Move images to the appropriate folders\n",
    "            for image in train_images:\n",
    "                shutil.copy(os.path.join(char_folder_path, image), os.path.join(train_folder, image))\n",
    "\n",
    "            for image in test_images:\n",
    "                shutil.copy(os.path.join(char_folder_path, image), os.path.join(test_folder, image))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'input_folder', 'output_train_folder', and 'output_test_folder' with your actual paths\n",
    "    input_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Preprocessed\"\n",
    "    output_train_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Train\"\n",
    "    output_test_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Test\"\n",
    "\n",
    "    split_data(input_folder, output_train_folder, output_test_folder, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f748cd6-a76b-4024-afb2-1b9184258c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(input_folder, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate through each subfolder (character class)\n",
    "    for char_folder in os.listdir(input_folder):\n",
    "        char_folder_path = os.path.join(input_folder, char_folder)\n",
    "\n",
    "        if os.path.isdir(char_folder_path):\n",
    "            output_char_folder = os.path.join(output_folder, char_folder)\n",
    "\n",
    "            # Create the output subfolder for the character class\n",
    "            if not os.path.exists(output_char_folder):\n",
    "                os.makedirs(output_char_folder)\n",
    "\n",
    "            # Iterate through each image in the character class subfolder\n",
    "            for filename in os.listdir(char_folder_path):\n",
    "                if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    # Read the image using Pillow\n",
    "                    image_path = os.path.join(char_folder_path, filename)\n",
    "\n",
    "                    img = Image.open(image_path).convert('L')  # Open and convert to grayscale\n",
    "                    img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "\n",
    "                    # Extract HOG features\n",
    "                    features, hog_image = hog(np.array(img), visualize=True)\n",
    "                    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "                    # Save the HOG image (for visualization purposes, optional)\n",
    "                    hog_image_path = os.path.join(output_char_folder, \"hog_\" + filename)\n",
    "                    Image.fromarray((hog_image_rescaled * 255).astype(np.uint8)).save(hog_image_path)\n",
    "\n",
    "                    # Save the HOG features as a NumPy array\n",
    "                    features_path = os.path.join(output_char_folder, \"features_\" + filename.replace(\".\", \"_\") + \".npy\")\n",
    "                    np.save(features_path, features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'input_folder' and 'output_folder' with your actual paths\n",
    "    input_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Train\"\n",
    "    output_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Features Train\"\n",
    "\n",
    "    preprocess_images(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daaea084-0e27-46d2-b999-0571e98414a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(input_folder, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate through each subfolder (character class)\n",
    "    for char_folder in os.listdir(input_folder):\n",
    "        char_folder_path = os.path.join(input_folder, char_folder)\n",
    "\n",
    "        if os.path.isdir(char_folder_path):\n",
    "            output_char_folder = os.path.join(output_folder, char_folder)\n",
    "\n",
    "            # Create the output subfolder for the character class\n",
    "            if not os.path.exists(output_char_folder):\n",
    "                os.makedirs(output_char_folder)\n",
    "\n",
    "            # Iterate through each image in the character class subfolder\n",
    "            for filename in os.listdir(char_folder_path):\n",
    "                if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    # Read the image using Pillow\n",
    "                    image_path = os.path.join(char_folder_path, filename)\n",
    "\n",
    "                    img = Image.open(image_path).convert('L')  # Open and convert to grayscale\n",
    "                    img = img.resize((28, 28))  # Resize to 28x28 pixels\n",
    "\n",
    "                    # Extract HOG features\n",
    "                    features, hog_image = hog(np.array(img), visualize=True)\n",
    "                    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "                    # Save the HOG image (for visualization purposes, optional)\n",
    "                    hog_image_path = os.path.join(output_char_folder, \"hog_\" + filename)\n",
    "                    Image.fromarray((hog_image_rescaled * 255).astype(np.uint8)).save(hog_image_path)\n",
    "\n",
    "                    # Save the HOG features as a NumPy array\n",
    "                    features_path = os.path.join(output_char_folder, \"features_\" + filename.replace(\".\", \"_\") + \".npy\")\n",
    "                    np.save(features_path, features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'input_folder' and 'output_folder' with your actual paths\n",
    "    input_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Test\"\n",
    "    output_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Features Test\"\n",
    "\n",
    "    preprocess_images(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0cefe81-b6b4-406d-8da6-bc723c743743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading features list and labels list separately as numpay array\n",
    "def load_data(folder):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for char_folder in os.listdir(folder):\n",
    "        char_folder_path = os.path.join(folder, char_folder)\n",
    "\n",
    "        if os.path.isdir(char_folder_path):\n",
    "            for filename in os.listdir(char_folder_path):\n",
    "                if filename.startswith(\"features_\") and filename.endswith(\".npy\"):\n",
    "                    features_path = os.path.join(char_folder_path, filename)\n",
    "                    label = char_folder  # Use folder name as label\n",
    "\n",
    "                    features = np.load(features_path)\n",
    "                    features_list.append(features)\n",
    "                    labels_list.append(label)\n",
    "\n",
    "    return np.array(features_list), np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05e3cd80-835e-439d-af32-41a6c57b7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for train data set\n",
    "input_train_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Features Train\"\n",
    "    \n",
    "# Load data features and labels from the training data set\n",
    "features_train, labels_train = load_data(input_train_folder)\n",
    "\n",
    "# Path for test data set\n",
    "input_test_folder = r\"C:\\Users\\Bizuhan\\Downloads\\OCR\\Amharic OCR Dataset Features Test\"\n",
    "\n",
    "# Load data features and labels from the test data set\n",
    "features_test, labels_test = load_data(input_test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "244ab63a-c67a-49e5-ad35-6bb9bcc8bff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 13.352826510721247 %\n"
     ]
    }
   ],
   "source": [
    "# Calculate the distance using euclidean metrics \n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "    \n",
    "# A function for OCR system using euclidean_distance measurement\n",
    "def nearest_neighbor(train_features, train_labels, test_feature):\n",
    "    distances = [euclidean_distance(train_feature, test_feature) for train_feature in train_features]\n",
    "    nearest_idx = np.argmin(distances)\n",
    "    return train_labels[nearest_idx]\n",
    "\n",
    "# Predict using euclidean distance\n",
    "predictions = [nearest_neighbor(features_train, labels_train, test_feature) for test_feature in features_test]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels_test, predictions) * 100\n",
    "print(\"Accuracy:\", accuracy, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42c3d0ae-dcb8-432f-b40c-4945eb8e761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 13.54775828460039 %\n"
     ]
    }
   ],
   "source": [
    "# Calculate the distance using manhattan metrics \n",
    "def manhattan_distance(x1, x2):\n",
    "    return np.sum(np.abs(x1 - x2))\n",
    "\n",
    "# A function for OCR system using manhattan_distance measurement\n",
    "def nearest_neighbor(train_features, train_labels, test_feature):\n",
    "    distances = [manhattan_distance(train_feature, test_feature) for train_feature in train_features]\n",
    "    nearest_idx = np.argmin(distances)\n",
    "    return train_labels[nearest_idx]\n",
    "    \n",
    "# Predict using manhattan distance\n",
    "predictions = [nearest_neighbor(features_train, labels_train, test_feature) for test_feature in features_test]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels_test, predictions) * 100\n",
    "print(\"Accuracy:\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74aeaffb-8f6b-462b-986d-8c0e796974bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 6.237816764132553 %\n"
     ]
    }
   ],
   "source": [
    "# Calculate the distance using chebyshev metrics \n",
    "def chebyshev_distance(x1, x2):\n",
    "    return np.max(np.abs(x1 - x2))\n",
    "\n",
    "# A function for OCR system using chebyshev_distance measurement\n",
    "def nearest_neighbor(train_features, train_labels, test_feature):\n",
    "    distances = [chebyshev_distance(train_feature, test_feature) for train_feature in train_features]\n",
    "    nearest_idx = np.argmin(distances)\n",
    "    return train_labels[nearest_idx]\n",
    "    \n",
    "# Predict using chebyshev distance\n",
    "predictions = [nearest_neighbor(features_train, labels_train, test_feature) for test_feature in features_test]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels_test, predictions) * 100\n",
    "print(\"Accuracy:\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e1db8e9-92fa-42ee-ae0b-7058a2860192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 13.352826510721247 %\n"
     ]
    }
   ],
   "source": [
    "# Calculate the distance using cosine metrics \n",
    "def cosine_distance(x1, x2):\n",
    "    dot_product = np.dot(x1, x2)\n",
    "    norm_x1 = np.linalg.norm(x1)\n",
    "    norm_x2 = np.linalg.norm(x2)\n",
    "    return 1 - (dot_product / (norm_x1 * norm_x2))\n",
    "\n",
    "# A function for OCR system using cosine_distance measurement\n",
    "def nearest_neighbor(train_features, train_labels, test_feature):\n",
    "    distances = [cosine_distance(train_feature, test_feature) for train_feature in train_features]\n",
    "    nearest_idx = np.argmin(distances)\n",
    "    return train_labels[nearest_idx]\n",
    "    \n",
    "# Predict using cosine distance\n",
    "predictions = [nearest_neighbor(features_train, labels_train, test_feature) for test_feature in features_test]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels_test, predictions) * 100\n",
    "print(\"Accuracy:\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a95d4-0708-4e49-9488-9a4bc3cb95e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
